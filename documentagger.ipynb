{
 "metadata": {
  "name": "",
  "signature": "sha256:2b05d365acc4578e6f6eca2c83af16657fd462e1987468299c45adf6b7c88624"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import nltk\n",
      "import re\n",
      "import os\n",
      "import codecs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata = pd.read_csv(\"DocumentTagResults.csv\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 215
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata.columns"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 216,
       "text": [
        "Index([u'DocumentID', u'Subject', u'FileName', u'TagName'], dtype='object')"
       ]
      }
     ],
     "prompt_number": 216
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "metadata['FileName'] = metadata['FileName'].replace(to_replace='.docx', value='.htm', regex=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 217
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "filenames = os.listdir('/Users/brandomr/Sites/docs')\n",
      "#gets list of filenames in the document directory\n",
      "\n",
      "len(filenames)\n",
      "#tests how many files are in the docs folder"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 218,
       "text": [
        "60"
       ]
      }
     ],
     "prompt_number": 218
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "corpora = []\n",
      "\n",
      "for i in filenames:\n",
      "    doc = open('/Users/brandomr/Sites/docs/'+ i)\n",
      "    text = doc.read()\n",
      "    #grabs the document as variable text\n",
      "    \n",
      "    text = nltk.clean_html(text)\n",
      "    #strips html formatting\n",
      "    \n",
      "    text = text.replace('&#xa0;', '\\xA0')\n",
      "    text = text.decode('utf-8', 'ignore')\n",
      "    #gets rid of non-break space html and converts to unicode\n",
      "    \n",
      "    corpora.append(text)\n",
      "    #adds to corpora"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 219
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#tokenizes and chunks for entity extraction\n",
      "def extract_entities(text):\n",
      "    entities = []\n",
      "    for sentence in nltk.sent_tokenize(text):\n",
      "        #tokenizes into sentence\n",
      "        \n",
      "        chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
      "        #tokenizes into words, then tags by parts of speech, then uses nltk's built in chunker\n",
      "        \n",
      "        entities.extend([chunk for chunk in chunks if hasattr(chunk, 'node')])\n",
      "        #iterates through the text and pulls any chunks that are nodes--these are the entities\n",
      "        \n",
      "    return entities\n",
      "\n",
      "#function takes two arguments, a list and the item you want entities for in that list. For example, with this corpora, select a given\n",
      "#item i for analysis--in this case i is just a document in the doc library already brought in\n",
      "def get_entitylist(list, i):\n",
      "    entitylist = []\n",
      "    for entity in extract_entities(list[i]):\n",
      "        #calls extract_entities on the given text\n",
      "        item = '[' + entity.node + '] ' + ' '.join(c[0] for c in entity.leaves())\n",
      "        #gets the entity node type and joins it with the leaf--basically the entity name in this case\n",
      "        entitylist.append(item)\n",
      "        #appends to the entitylist object\n",
      "    return entitylist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 283
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#uniq is a function to return only unique items for a given list\n",
      "def uniq(input):\n",
      "    output = []\n",
      "    for x in input: \n",
      "        if x not in output:\n",
      "            output.append(x)\n",
      "    return output"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 275
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#prints each item in a list on it's own line\n",
      "def printbyline(list):\n",
      "    for item in list:\n",
      "        print item"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 286
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "entitysample = uniq(get_entitylist(corpora, 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 287
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "printbyline(sorted(entitysample))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[FACILITY] Millennium Challenge Corporation\n",
        "[FACILITY] White House\n",
        "[GPE] Action\n",
        "[GPE] American\n",
        "[GPE] Bali\n",
        "[GPE] Cancun\n",
        "[GPE] China\n",
        "[GPE] Copenhagen\n",
        "[GPE] Doha\n",
        "[GPE] Domestic\n",
        "[GPE] Durban\n",
        "[GPE] EU\n",
        "[GPE] First\n",
        "[GPE] Focus\n",
        "[GPE] France\n",
        "[GPE] India\n",
        "[GPE] Kyoto\n",
        "[GPE] Left\n",
        "[GPE] Main\n",
        "[GPE] Nor\n",
        "[GPE] Poland\n",
        "[GPE] Second\n",
        "[GPE] Seoul\n",
        "[GPE] State\n",
        "[GPE] Th\n",
        "[GPE] U.S.\n",
        "[GPE] United States\n",
        "[GPE] Washington\n",
        "[LOCATION] Clean Energy\n",
        "[ORGANIZATION] Administration\n",
        "[ORGANIZATION] Areas\n",
        "[ORGANIZATION] Bank\n",
        "[ORGANIZATION] CBDR\n",
        "[ORGANIZATION] CCAC\n",
        "[ORGANIZATION] CO2\n",
        "[ORGANIZATION] Climate\n",
        "[ORGANIZATION] Conference\n",
        "[ORGANIZATION] Congress\n",
        "[ORGANIZATION] Copenhagen\n",
        "[ORGANIZATION] Disabilities Convention\n",
        "[ORGANIZATION] Durban Platform\n",
        "[ORGANIZATION] EXIM\n",
        "[ORGANIZATION] Energy\n",
        "[ORGANIZATION] Fast Start\n",
        "[ORGANIZATION] Green Climate Fund\n",
        "[ORGANIZATION] HFCs\n",
        "[ORGANIZATION] Initiatives\n",
        "[ORGANIZATION] Kyoto\n",
        "[ORGANIZATION] LCA\n",
        "[ORGANIZATION] MEF\n",
        "[ORGANIZATION] MEF Action Agenda\n",
        "[ORGANIZATION] Major\n",
        "[ORGANIZATION] Overseas Private Investment Corporation\n",
        "[ORGANIZATION] Parties\n",
        "[ORGANIZATION] SSECC Policy Overview\n",
        "[ORGANIZATION] Senate\n",
        "[ORGANIZATION] Special Envoy\n",
        "[ORGANIZATION] State\n",
        "[ORGANIZATION] State Department\n",
        "[ORGANIZATION] Technology Center\n",
        "[ORGANIZATION] Treasury\n",
        "[ORGANIZATION] U\n",
        "[ORGANIZATION] UN Environmental Program\n",
        "[ORGANIZATION] UN Framework Convention\n",
        "[ORGANIZATION] UNFCCC\n",
        "[ORGANIZATION] USAID\n",
        "[ORGANIZATION] World Bank\n",
        "[PERSON] Brazil\n",
        "[PERSON] Bush\n",
        "[PERSON] Celsius\n",
        "[PERSON] Clean Air Coalition\n",
        "[PERSON] Climate\n",
        "[PERSON] Climate Change\n",
        "[PERSON] Clinton\n",
        "[PERSON] Concrete\n",
        "[PERSON] Green Climate Fund\n",
        "[PERSON] Network\n",
        "[PERSON] Obama\n",
        "[PERSON] Policy Overview\n",
        "[PERSON] Special Envoy Stern\n",
        "[PERSON] Special Envoy Todd Stern\n",
        "[PERSON] Todd Stern\n"
       ]
      }
     ],
     "prompt_number": 278
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sampletext = ['Brandon is the bomb diggety fresh. He lives in Washington DC in a small apartment with Barb. He loves her very much and likes going to work at the State Department. One day he will be rich and famous and have houses in New York City and San Francisco and Hong Kong and Los Angeles and San Diego.']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 288
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_entitylist(sampletext,0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 289,
       "text": [
        "['[GPE] Brandon',\n",
        " '[GPE] Washington',\n",
        " '[PERSON] Barb',\n",
        " '[ORGANIZATION] State Department',\n",
        " '[GPE] New York City',\n",
        " '[PERSON] San Francisco',\n",
        " '[GPE] Hong Kong',\n",
        " '[GPE] Los Angeles',\n",
        " '[PERSON] San Diego']"
       ]
      }
     ],
     "prompt_number": 289
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}